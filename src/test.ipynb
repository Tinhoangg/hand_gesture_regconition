{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2599afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "565cf0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ------------------ Dataset ------------------\n",
    "class HandSignDataset(Dataset):\n",
    "    def __init__(self, root_dir, num_frames=60):\n",
    "        self.samples = []\n",
    "        self.labels = []\n",
    "        self.label_names = sorted(os.listdir(root_dir))\n",
    "        self.num_frames = num_frames\n",
    "\n",
    "        for label_idx, label in enumerate(self.label_names):\n",
    "            label_folder = os.path.join(root_dir, label)\n",
    "            if not os.path.isdir(label_folder):\n",
    "                continue\n",
    "            for file in os.listdir(label_folder):\n",
    "                if file.endswith('.npy'):\n",
    "                    path = os.path.join(label_folder, file)\n",
    "                    self.samples.append(path)\n",
    "                    self.labels.append(label_idx)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = np.load(self.samples[idx])\n",
    "\n",
    "        # Pad hoặc cắt số frame\n",
    "        if data.shape[0] < self.num_frames:\n",
    "            pad = np.zeros((self.num_frames - data.shape[0], 2, 21, 2))\n",
    "            data = np.concatenate([data, pad], axis=0)\n",
    "        else:\n",
    "            data = data[:self.num_frames]\n",
    "\n",
    "        data = data.reshape(self.num_frames, -1)  # (60, 84)\n",
    "        label = self.labels[idx]\n",
    "        return torch.tensor(data, dtype=torch.float32), torch.tensor(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb487192",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HandSignTransformer(nn.Module):\n",
    "    def __init__(self, input_size=84, num_classes=25, hidden_dim=128, num_heads=4, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.input_proj = nn.Linear(input_size, hidden_dim)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=num_heads, dropout=0.2, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_proj(x)  # (B, seq, hidden)\n",
    "        x = self.transformer(x)\n",
    "        out = x.mean(dim=1)     # average pooling theo thời gian\n",
    "        return self.fc(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4778f06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ Train function ------------------\n",
    "def train_transformer(train_dir, val_dir, epochs=20, batch_size=8, lr=1e-3):\n",
    "    # Dataset\n",
    "    train_dataset = HandSignDataset(train_dir)\n",
    "    val_dataset = HandSignDataset(val_dir)\n",
    "\n",
    "    num_classes = len(train_dataset.label_names)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Model (bạn cần định nghĩa HandSignTransformer trước)\n",
    "    model = HandSignTransformer(num_classes=num_classes)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "    # Train loop\n",
    "    for epoch in range(epochs):\n",
    "        # ---- TRAIN ----\n",
    "        model.train()\n",
    "        total_loss, correct, total = 0, 0, 0\n",
    "        for x, y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            out = model(x)\n",
    "            loss = criterion(out, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            correct += (out.argmax(1) == y).sum().item()\n",
    "            total += y.size(0)\n",
    "        train_acc = 100 * correct / total\n",
    "\n",
    "        # ---- VALIDATION ----\n",
    "        model.eval()\n",
    "        val_loss, val_correct, val_total = 0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for x, y in val_loader:\n",
    "                out = model(x)\n",
    "                loss = criterion(out, y)\n",
    "                val_loss += loss.item()\n",
    "                val_correct += (out.argmax(1) == y).sum().item()\n",
    "                val_total += y.size(0)\n",
    "        val_acc = 100 * val_correct / val_total\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] \"\n",
    "              f\"| Train Loss: {total_loss:.4f} | Train Acc: {train_acc:.2f}% \"\n",
    "              f\"| Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "    # Lưu model\n",
    "    torch.save({\n",
    "        'model_state': model.state_dict(),\n",
    "        'labels': train_dataset.label_names\n",
    "    }, \"hand_transformer.pth\")\n",
    "    print(\" Model saved as hand_transformer.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c09f58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import torch\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_drawing_pose = mp.solutions.drawing_utils\n",
    "\n",
    "\n",
    "# --- Hàm lấy cổ và khoảng cách vai để chuẩn hoá ---\n",
    "def get_neck_point(frame):\n",
    "    pose = mp_pose.Pose(static_image_mode=False,\n",
    "                        min_detection_confidence=0.6,\n",
    "                        min_tracking_confidence=0.5)\n",
    "\n",
    "    image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = pose.process(image)\n",
    "    pose.close()\n",
    "\n",
    "    if not results.pose_landmarks:\n",
    "        return np.array([0, 0]), 1.0\n",
    "\n",
    "    keypoints = np.array([[lm.x, lm.y, lm.z] for lm in results.pose_landmarks.landmark])\n",
    "    shoulder_right = keypoints[11][:2]\n",
    "    shoulder_left = keypoints[12][:2]\n",
    "    neck = (shoulder_left + shoulder_right) / 2\n",
    "    shoulder_dist = np.linalg.norm(shoulder_left - shoulder_right)\n",
    "    return neck, shoulder_dist if shoulder_dist > 0 else 1.0\n",
    "\n",
    "\n",
    "# --- Chuẩn hoá keypoint ---\n",
    "def normalize_keypoints(keypoint, neck_point, shoulder_dist):\n",
    "    normalized = np.zeros_like(keypoint)\n",
    "    for i in range(2):\n",
    "        hand = keypoint[i]\n",
    "        if np.all(hand == 0):\n",
    "            continue\n",
    "        rel = hand - neck_point\n",
    "        normalized[i] = rel / shoulder_dist\n",
    "    return normalized\n",
    "\n",
    "\n",
    "# --- Hàm realtime ---\n",
    "def test_realtime(model_path=\"hand_transformer.pth\", seq_length=30, conf_thresh=0.8):\n",
    "    checkpoint = torch.load(model_path, map_location=torch.device('cpu'))\n",
    "    labels = checkpoint['labels']\n",
    "\n",
    "    model = HandSignTransformer(num_classes=len(labels))\n",
    "    model.load_state_dict(checkpoint['model_state'])\n",
    "    model.eval()\n",
    "\n",
    "    hands = mp_hands.Hands(max_num_hands=2,\n",
    "                           min_detection_confidence=0.5,\n",
    "                           min_tracking_confidence=0.5)\n",
    "\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    sequence = []\n",
    "    last_label = \"...\"\n",
    "\n",
    "    print(\"Bắt đầu nhận diện realtime (nhấn Q để thoát)\")\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # --- Tính cổ và vai từ frame ---\n",
    "        neck_point, shoulder_dist = get_neck_point(frame)\n",
    "        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = hands.process(rgb)\n",
    "\n",
    "        # --- Trích xuất keypoints bàn tay ---\n",
    "        keypoints = np.zeros((2, 21, 2))\n",
    "        if results.multi_hand_landmarks:\n",
    "            for i, hand in enumerate(results.multi_hand_landmarks[:2]):\n",
    "                for j, lm in enumerate(hand.landmark):\n",
    "                    keypoints[i, j] = [lm.x, lm.y]\n",
    "                mp.solutions.drawing_utils.draw_landmarks(frame, hand, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "        # --- Chuẩn hoá ---\n",
    "        normalized_kp = normalize_keypoints(keypoints, neck_point, shoulder_dist)\n",
    "        sequence.append(normalized_kp)\n",
    "        if len(sequence) > seq_length:\n",
    "            sequence.pop(0)\n",
    "\n",
    "        # --- Nếu đủ frame thì predict ---\n",
    "        if len(sequence) == seq_length:\n",
    "            data = np.array(sequence).reshape(seq_length, -1)\n",
    "\n",
    "            # Bỏ qua nếu tay không có (toàn 0)\n",
    "            if np.sum(np.abs(data)) < 1e-4:\n",
    "                label = \"No gesture\"\n",
    "            else:\n",
    "                x = torch.tensor(data, dtype=torch.float32).unsqueeze(0)\n",
    "                with torch.no_grad():\n",
    "                    preds = torch.softmax(model(x), dim=1)\n",
    "                    conf, pred_idx = torch.max(preds, dim=1)\n",
    "                    if conf.item() < conf_thresh:\n",
    "                        label = \"No gesture\"\n",
    "                    else:\n",
    "                        label = labels[pred_idx.item()]\n",
    "                        last_label = label\n",
    "\n",
    "            cv2.putText(frame, f\"{label}\", (20, 50),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 0), 3)\n",
    "\n",
    "        cv2.imshow(\"Hand Sign Transformer\", frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb86c364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20] | Train Loss: 77.3010 | Train Acc: 50.74% | Val Loss: 8.4710 | Val Acc: 80.62%\n",
      "Epoch [2/20] | Train Loss: 25.9698 | Train Acc: 86.94% | Val Loss: 3.3801 | Val Acc: 91.25%\n",
      "Epoch [3/20] | Train Loss: 12.6706 | Train Acc: 92.26% | Val Loss: 2.0563 | Val Acc: 95.94%\n",
      "Epoch [4/20] | Train Loss: 7.8120 | Train Acc: 96.01% | Val Loss: 1.7609 | Val Acc: 95.94%\n",
      "Epoch [5/20] | Train Loss: 6.3665 | Train Acc: 96.79% | Val Loss: 1.4172 | Val Acc: 96.25%\n",
      "Epoch [6/20] | Train Loss: 3.1784 | Train Acc: 98.83% | Val Loss: 1.0355 | Val Acc: 97.81%\n",
      "Epoch [7/20] | Train Loss: 4.7026 | Train Acc: 96.33% | Val Loss: 1.6938 | Val Acc: 95.62%\n",
      "Epoch [8/20] | Train Loss: 4.8708 | Train Acc: 97.19% | Val Loss: 1.0841 | Val Acc: 96.56%\n",
      "Epoch [9/20] | Train Loss: 3.7539 | Train Acc: 97.65% | Val Loss: 0.7073 | Val Acc: 98.75%\n",
      "Epoch [10/20] | Train Loss: 2.6301 | Train Acc: 98.67% | Val Loss: 0.8229 | Val Acc: 97.19%\n",
      "Epoch [11/20] | Train Loss: 2.6782 | Train Acc: 98.20% | Val Loss: 0.8124 | Val Acc: 96.25%\n",
      "Epoch [12/20] | Train Loss: 2.8225 | Train Acc: 98.20% | Val Loss: 1.1410 | Val Acc: 96.25%\n",
      "Epoch [13/20] | Train Loss: 3.8079 | Train Acc: 96.64% | Val Loss: 0.9322 | Val Acc: 96.56%\n",
      "Epoch [14/20] | Train Loss: 2.2707 | Train Acc: 98.44% | Val Loss: 0.4877 | Val Acc: 98.75%\n",
      "Epoch [15/20] | Train Loss: 3.3236 | Train Acc: 97.97% | Val Loss: 0.7040 | Val Acc: 97.19%\n",
      "Epoch [16/20] | Train Loss: 2.6935 | Train Acc: 97.81% | Val Loss: 1.0326 | Val Acc: 96.88%\n",
      "Epoch [17/20] | Train Loss: 2.6378 | Train Acc: 97.97% | Val Loss: 0.6177 | Val Acc: 98.75%\n",
      "Epoch [18/20] | Train Loss: 2.4624 | Train Acc: 97.97% | Val Loss: 0.6448 | Val Acc: 97.81%\n",
      "Epoch [19/20] | Train Loss: 1.4151 | Train Acc: 98.83% | Val Loss: 0.3543 | Val Acc: 99.06%\n",
      "Epoch [20/20] | Train Loss: 0.8031 | Train Acc: 99.22% | Val Loss: 0.3220 | Val Acc: 98.75%\n",
      " Model saved as hand_transformer.pth\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "train_transformer('D:/Semester/Semester5/DPL302/Project/data_split/train','D:/Semester/Semester5/DPL302/Project/data_split/val', epochs=20, batch_size=32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2faace67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bắt đầu nhận diện realtime (nhấn Q để thoát)\n"
     ]
    }
   ],
   "source": [
    "# Test real-time\n",
    "test_realtime(\"hand_transformer.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
