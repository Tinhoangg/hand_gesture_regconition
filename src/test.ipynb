{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac2ad920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_transformer_ctc_best.py\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "from collections import Counter\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from tqdm import tqdm\n",
    "\n",
    "# -----------------------\n",
    "# Config (basic)\n",
    "# -----------------------\n",
    "class CFG:\n",
    "    data_dir = \"D:/Semester/Semester5/DPL302/Project/sentence_dataset\"\n",
    "    meta = \"D:/Semester/Semester5/DPL302/Project/metadata.json\"\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    epochs = 40\n",
    "    batch_size = 16\n",
    "    max_frames = 600            # pad/truncate length (frames)\n",
    "    subsample_factor = 4        # conv subsample factor\n",
    "    d_model = 256\n",
    "    nhead = 8\n",
    "    num_layers = 4\n",
    "    dropout = 0.1\n",
    "    lr = 2e-4\n",
    "    weight_decay = 1e-4\n",
    "    clip_grad = 1.0\n",
    "    temperature = 0.6           # <1 to sharpen logits\n",
    "    blank_penalty = 0.0         # small positive to discourage blank collapse e.g. 0.01\n",
    "    num_workers = 2\n",
    "    seed = 42\n",
    "    save_path = \"best_transformer_ctc_best.pth\"\n",
    "    use_amp = True\n",
    "    balanced_batch = True       # use balanced sampling by token frequency\n",
    "    shuffle = True\n",
    "\n",
    "# -----------------------\n",
    "# Utilities\n",
    "# -----------------------\n",
    "def set_seed(s):\n",
    "    random.seed(s); np.random.seed(s); torch.manual_seed(s)\n",
    "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a20edef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Tokenizer (space-tokenized labels)\n",
    "# -----------------------\n",
    "class Tokenizer:\n",
    "    def __init__(self, sequences: List[List[str]]):\n",
    "        tokens = sorted({t for seq in sequences for t in seq})\n",
    "        self.blank = \"<BLANK>\"\n",
    "        self.idx_to_token = [self.blank] + tokens\n",
    "        self.token_to_idx = {t:i for i,t in enumerate(self.idx_to_token)}\n",
    "        self.blank_idx = 0\n",
    "\n",
    "    def encode(self, seq: List[str]) -> List[int]:\n",
    "        return [self.token_to_idx[t] for t in seq]\n",
    "\n",
    "    def decode(self, indices: List[int]) -> str:\n",
    "        out = []\n",
    "        prev = None\n",
    "        for i in indices:\n",
    "            if i == prev: \n",
    "                continue\n",
    "            if i == self.blank_idx:\n",
    "                prev = i; continue\n",
    "            out.append(self.idx_to_token[i]); prev = i\n",
    "        return \" \".join(out)\n",
    "\n",
    "# -----------------------\n",
    "# Dataset\n",
    "# -----------------------\n",
    "class SentenceDataset(Dataset):\n",
    "    def __init__(self, data_dir, meta_path, tokenizer, max_frames):\n",
    "        with open(meta_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            self.meta = json.load(f)   # id -> [labels]\n",
    "        self.ids = sorted(self.meta.keys())\n",
    "        self.data_dir = data_dir\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_frames = max_frames\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sid = self.ids[idx]\n",
    "        arr = np.load(os.path.join(self.data_dir, f\"{sid}.npy\")).astype(np.float32)  # (T,D)\n",
    "        T, D = arr.shape\n",
    "        # already normalized per your comment\n",
    "        # truncate/pad later in collate\n",
    "        target = np.array(self.tokenizer.encode(self.meta[sid]), dtype=np.int32)\n",
    "        return arr, target\n",
    "\n",
    "    def collate_fn(batch, max_frames):\n",
    "        seqs, targets = zip(*batch)\n",
    "        B = len(seqs)\n",
    "        D = seqs[0].shape[1]\n",
    "        padded = np.zeros((B, max_frames, D), dtype=np.float32)\n",
    "        input_lengths = []\n",
    "        tgt_list = []\n",
    "        tgt_lens = []\n",
    "        for i, s in enumerate(seqs):\n",
    "            L = min(s.shape[0], max_frames)\n",
    "            padded[i, :L, :] = s[:L, :]\n",
    "            input_lengths.append(L)\n",
    "        for t in targets:\n",
    "            tgt_list.append(t)\n",
    "            tgt_lens.append(len(t))\n",
    "        if len(tgt_list) > 0:\n",
    "            targets_concat = np.concatenate(tgt_list).astype(np.int32)\n",
    "        else:\n",
    "            targets_concat = np.array([], dtype=np.int32)\n",
    "        return (torch.from_numpy(padded), torch.tensor(input_lengths, dtype=torch.long),\n",
    "                torch.from_numpy(targets_concat).long(), torch.tensor(tgt_lens, dtype=torch.long))\n",
    "\n",
    "# -----------------------\n",
    "# Balanced sampler by token frequency (batch balanced roughly by number of tokens in batch)\n",
    "# -----------------------\n",
    "class BalancedSampler(Sampler):\n",
    "    def __init__(self, meta, batch_size):\n",
    "        # meta: id->list labels\n",
    "        # build index per token\n",
    "        token_to_ids = {}\n",
    "        for sid, seq in meta.items():\n",
    "            for t in seq:\n",
    "                token_to_ids.setdefault(t, []).append(sid)\n",
    "        self.token_to_ids = token_to_ids\n",
    "        self.ids = list(meta.keys())\n",
    "        self.batch_size = batch_size\n",
    "        self.meta = meta\n",
    "\n",
    "    def __iter__(self):\n",
    "        # build batches by selecting token types uniformly then choosing ids containing them\n",
    "        ids_chosen = []\n",
    "        token_list = list(self.token_to_ids.keys())\n",
    "        # Round-robin across tokens to reach all ids roughly balanced\n",
    "        idx = 0\n",
    "        while len(ids_chosen) < len(self.ids):\n",
    "            t = token_list[idx % len(token_list)]\n",
    "            pool = self.token_to_ids[t]\n",
    "            pick = random.choice(pool)\n",
    "            if pick not in ids_chosen:\n",
    "                ids_chosen.append(pick)\n",
    "            idx += 1\n",
    "        # pad to multiple of batch_size\n",
    "        while len(ids_chosen) % self.batch_size != 0:\n",
    "            ids_chosen.append(random.choice(self.ids))\n",
    "        # yield batches\n",
    "        for i in range(0, len(ids_chosen), self.batch_size):\n",
    "            yield ids_chosen[i:i+self.batch_size]\n",
    "\n",
    "    def __len__(self):\n",
    "        return math.ceil(len(self.ids) / self.batch_size)\n",
    "\n",
    "# -----------------------\n",
    "# Model: Conv subsample + Transformer encoder + linear\n",
    "# -----------------------\n",
    "class ConvSubsample(nn.Module):\n",
    "    def __init__(self, in_dim, d_model, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(in_dim, d_model, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv1d(d_model, d_model, kernel_size=3, stride=2, padding=1)\n",
    "        self.act = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        # x: (B, T, D)\n",
    "        x = x.transpose(1,2)   # (B, D, T)\n",
    "        x = self.conv1(x); x = self.act(x)\n",
    "        x = self.conv2(x); x = self.act(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x.transpose(1,2)   # (B, T_sub, d_model)\n",
    "        return x\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=2000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div)\n",
    "        pe[:, 1::2] = torch.cos(position * div)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "    def forward(self, x):\n",
    "        # x: (B, T, D)\n",
    "        T = x.size(1)\n",
    "        return x + self.pe[:, :T, :]\n",
    "\n",
    "class TransformerCTC(nn.Module):\n",
    "    def __init__(self, in_dim, num_classes, d_model=256, nhead=8, num_layers=4, dropout=0.1, use_subsample=True, temp=1.0):\n",
    "        super().__init__()\n",
    "        self.use_subsample = use_subsample\n",
    "        self.sub = ConvSubsample(in_dim, d_model) if use_subsample else None\n",
    "        self.input_proj = nn.Linear(in_dim, d_model) if not use_subsample else None\n",
    "        self.pos = PositionalEncoding(d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward=4*d_model, dropout=dropout)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.out = nn.Linear(d_model, num_classes)\n",
    "        self.d_model = d_model\n",
    "        self.temp = temp\n",
    "    def forward(self, x, src_key_padding_mask=None):\n",
    "        # x: (B, T, D)\n",
    "        if self.sub is not None:\n",
    "            x = self.sub(x)    # (B, T_sub, d_model)\n",
    "        else:\n",
    "            x = self.input_proj(x) * math.sqrt(self.d_model)\n",
    "        x = self.pos(x)\n",
    "        x = x.transpose(0,1)  # (T, B, D)\n",
    "        enc = self.encoder(x, src_key_padding_mask=src_key_padding_mask)\n",
    "        logits = self.out(enc) / (self.temp if self.temp>0 else 1.0)\n",
    "        return F.log_softmax(logits, dim=-1)   # (T', B, C)\n",
    "\n",
    "# -----------------------\n",
    "# Helpers: masks, decode\n",
    "# -----------------------\n",
    "def make_src_key_padding_mask(lengths, max_len, subsample=4):\n",
    "    # lengths: tensor (B,) original lengths\n",
    "    B = lengths.size(0)\n",
    "    out_len = (max_len + subsample - 1) // subsample\n",
    "    mask = torch.zeros((B, out_len), dtype=torch.bool)\n",
    "    for i, L in enumerate(lengths.tolist()):\n",
    "        newL = (L + subsample - 1) // subsample\n",
    "        if newL < out_len:\n",
    "            mask[i, newL:] = True\n",
    "    return mask\n",
    "\n",
    "def greedy_decode(log_probs, tokenizer):\n",
    "    # log_probs: (T, B, C)\n",
    "    probs = log_probs.exp()\n",
    "    arg = probs.argmax(dim=-1).cpu().numpy()  # (T, B)\n",
    "    preds = []\n",
    "    blank_probs = []\n",
    "    for b in range(arg.shape[1]):\n",
    "        seq = arg[:, b].tolist()\n",
    "        prev = None\n",
    "        out = []\n",
    "        for idx in seq:\n",
    "            if idx == prev:\n",
    "                continue\n",
    "            if idx == tokenizer.blank_idx:\n",
    "                prev = idx; continue\n",
    "            out.append(idx); prev = idx\n",
    "        preds.append(tokenizer.decode(out))\n",
    "        blank_probs.append(probs[:, b, tokenizer.blank_idx].mean().item())\n",
    "    return preds, sum(blank_probs)/len(blank_probs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01febdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Training / Validation\n",
    "# -----------------------\n",
    "def train_one_epoch(model, loader, optimizer, scaler, ctc_loss_fn, cfg, tokenizer):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_blank = 0.0\n",
    "    for X, input_lens, targets, target_lens in tqdm(loader, desc=\"Train\"):\n",
    "        X = X.to(cfg.device); input_lens = input_lens.to(cfg.device)\n",
    "        targets = targets.to(cfg.device); target_lens = target_lens.to(cfg.device)\n",
    "        mask = make_src_key_padding_mask(input_lens, cfg.max_frames, subsample=cfg.subsample_factor).to(cfg.device)\n",
    "        optimizer.zero_grad()\n",
    "        with autocast(enabled=cfg.use_amp):\n",
    "            logp = model(X, src_key_padding_mask=mask)  # (T', B, C)\n",
    "            input_lens_ctc = ((input_lens + cfg.subsample_factor - 1)//cfg.subsample_factor).clamp(min=1)\n",
    "            loss = ctc_loss_fn(logp, targets, input_lens_ctc, target_lens)\n",
    "            if cfg.blank_penalty > 0:\n",
    "                blank_prob = logp.exp()[:, :, 0].mean()\n",
    "                loss = loss + cfg.blank_penalty * blank_prob\n",
    "        scaler.scale(loss).backward()\n",
    "        if cfg.clip_grad > 0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.clip_grad)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        total_loss += loss.item() * X.size(0)\n",
    "        # blank prob reporting\n",
    "        with torch.no_grad():\n",
    "            bprob = logp.exp()[:, :, 0].mean().item()\n",
    "            total_blank += bprob * X.size(0)\n",
    "    return total_loss / len(loader.dataset), total_blank / len(loader.dataset)\n",
    "\n",
    "def validate(model, loader, ctc_loss_fn, cfg, tokenizer):\n",
    "    model.eval()\n",
    "    total = 0; correct = 0; blanks = 0.0; tot_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X, input_lens, targets, target_lens in tqdm(loader, desc=\"Val\"):\n",
    "            X = X.to(cfg.device); input_lens = input_lens.to(cfg.device)\n",
    "            targets = targets.to(cfg.device); target_lens = target_lens.to(cfg.device)\n",
    "            mask = make_src_key_padding_mask(input_lens, cfg.max_frames, subsample=cfg.subsample_factor).to(cfg.device)\n",
    "            logp = model(X, src_key_padding_mask=mask)\n",
    "            input_lens_ctc = ((input_lens + cfg.subsample_factor - 1)//cfg.subsample_factor).clamp(min=1)\n",
    "            loss = ctc_loss_fn(logp, targets, input_lens_ctc, target_lens)\n",
    "            tot_loss += loss.item() * X.size(0)\n",
    "            preds, avg_blank = greedy_decode(logp, tokenizer)\n",
    "            blanks += avg_blank * X.size(0)\n",
    "            # reconstruct true strings\n",
    "            ptr = 0\n",
    "            for b in range(X.size(0)):\n",
    "                L = target_lens[b].item()\n",
    "                true_inds = targets[ptr:ptr+L].cpu().numpy().tolist()\n",
    "                true_str = tokenizer.decode(true_inds)\n",
    "                ptr += L\n",
    "                if preds[b].strip() == true_str.strip():\n",
    "                    correct += 1\n",
    "                total += 1\n",
    "    acc = correct / total if total>0 else 0.0\n",
    "    return tot_loss/len(loader.dataset), acc, blanks/len(loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9bd79de9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 67\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     66\u001b[0m     cfg \u001b[38;5;241m=\u001b[39m CFG()\n\u001b[1;32m---> 67\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[14], line 36\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(cfg)\u001b[0m\n\u001b[0;32m     32\u001b[0m val_loader \u001b[38;5;241m=\u001b[39m DataLoader(val_ds, batch_size\u001b[38;5;241m=\u001b[39mcfg\u001b[38;5;241m.\u001b[39mbatch_size, collate_fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m b: collate_fn(b, cfg\u001b[38;5;241m.\u001b[39mmax_frames),\n\u001b[0;32m     33\u001b[0m                         num_workers\u001b[38;5;241m=\u001b[39mcfg\u001b[38;5;241m.\u001b[39mnum_workers, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# model\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m sample_arr, _ \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     37\u001b[0m in_dim \u001b[38;5;241m=\u001b[39m sample_arr\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     38\u001b[0m model \u001b[38;5;241m=\u001b[39m TransformerCTC(in_dim, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(tokenizer\u001b[38;5;241m.\u001b[39midx_to_token),\n\u001b[0;32m     39\u001b[0m                        d_model\u001b[38;5;241m=\u001b[39mcfg\u001b[38;5;241m.\u001b[39md_model, nhead\u001b[38;5;241m=\u001b[39mcfg\u001b[38;5;241m.\u001b[39mnhead, num_layers\u001b[38;5;241m=\u001b[39mcfg\u001b[38;5;241m.\u001b[39mnum_layers,\n\u001b[0;32m     40\u001b[0m                        dropout\u001b[38;5;241m=\u001b[39mcfg\u001b[38;5;241m.\u001b[39mdropout, use_subsample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, temp\u001b[38;5;241m=\u001b[39mcfg\u001b[38;5;241m.\u001b[39mtemperature)\n",
      "Cell \u001b[1;32mIn[3], line 44\u001b[0m, in \u001b[0;36mSentenceDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     42\u001b[0m sid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mids[idx]\n\u001b[0;32m     43\u001b[0m arr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msid\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m))\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)  \u001b[38;5;66;03m# (T,D)\u001b[39;00m\n\u001b[1;32m---> 44\u001b[0m T, D \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# already normalized per your comment\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# truncate/pad later in collate\u001b[39;00m\n\u001b[0;32m     47\u001b[0m target \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeta[sid]), dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint32)\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "# -----------------------\n",
    "# Main\n",
    "# -----------------------\n",
    "def main(cfg: CFG):\n",
    "    set_seed(cfg.seed)\n",
    "    # load meta\n",
    "    with open(cfg.meta, \"r\", encoding=\"utf-8\") as f:\n",
    "        meta = json.load(f)  # id -> [labels]\n",
    "    sequences = list(meta.values())\n",
    "    tokenizer = Tokenizer(sequences)\n",
    "    dataset = SentenceDataset(cfg.data_dir, cfg.meta, tokenizer, cfg.max_frames)\n",
    "\n",
    "    # split train/val\n",
    "    ids = dataset.ids\n",
    "    random.shuffle(ids)\n",
    "    n_train = int(0.8 * len(ids))\n",
    "    train_ids = set(ids[:n_train])\n",
    "    train_meta = {k:v for k,v in meta.items() if k in train_ids}\n",
    "    val_meta = {k:v for k,v in meta.items() if k not in train_ids}\n",
    "\n",
    "    train_ds = SentenceDataset(cfg.data_dir, cfg.meta, tokenizer, cfg.max_frames)\n",
    "    val_ds = SentenceDataset(cfg.data_dir, cfg.meta, tokenizer, cfg.max_frames)\n",
    "\n",
    "    # dataloaders\n",
    "    if cfg.balanced_batch:\n",
    "        sampler = BalancedSampler(meta, cfg.batch_size)\n",
    "        train_loader = DataLoader(train_ds, batch_size=cfg.batch_size, collate_fn=lambda b: collate_fn(b, cfg.max_frames),\n",
    "                                  num_workers=cfg.num_workers, sampler=sampler)\n",
    "    else:\n",
    "        train_loader = DataLoader(train_ds, batch_size=cfg.batch_size, collate_fn=lambda b: collate_fn(b, cfg.max_frames),\n",
    "                                  num_workers=cfg.num_workers, shuffle=cfg.shuffle)\n",
    "    val_loader = DataLoader(val_ds, batch_size=cfg.batch_size, collate_fn=lambda b: collate_fn(b, cfg.max_frames),\n",
    "                            num_workers=cfg.num_workers, shuffle=False)\n",
    "\n",
    "    # model\n",
    "    sample_arr, _ = dataset[0]\n",
    "    in_dim = sample_arr.shape[1]\n",
    "    model = TransformerCTC(in_dim, num_classes=len(tokenizer.idx_to_token),\n",
    "                           d_model=cfg.d_model, nhead=cfg.nhead, num_layers=cfg.num_layers,\n",
    "                           dropout=cfg.dropout, use_subsample=True, temp=cfg.temperature)\n",
    "    model.to(cfg.device)\n",
    "\n",
    "    ctc_loss_fn = nn.CTCLoss(blank=tokenizer.blank_idx, zero_infinity=True, reduction=\"mean\")\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3, verbose=True)\n",
    "    scaler = GradScaler(enabled=cfg.use_amp)\n",
    "\n",
    "    best_acc = 0.0\n",
    "    for epoch in range(1, cfg.epochs+1):\n",
    "        tr_loss, tr_blank = train_one_epoch(model, train_loader, optimizer, scaler, ctc_loss_fn, cfg, tokenizer)\n",
    "        val_loss, val_acc, val_blank = validate(model, val_loader, ctc_loss_fn, cfg, tokenizer)\n",
    "        scheduler.step(val_acc)\n",
    "        print(f\"[Epoch {epoch}] tr_loss={tr_loss:.4f} tr_blank={tr_blank:.3f} | val_loss={val_loss:.4f} val_acc={val_acc:.4f} val_blank={val_blank:.3f}\")\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            torch.save({\n",
    "                \"model_state\": model.state_dict(),\n",
    "                \"tokenizer\": tokenizer.__dict__,\n",
    "                \"cfg\": cfg.__dict__,\n",
    "                \"epoch\": epoch,\n",
    "                \"val_acc\": val_acc\n",
    "                }, cfg.save_path)\n",
    "            print(\"Saved best model:\", cfg.save_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    cfg = CFG()\n",
    "    main(cfg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
